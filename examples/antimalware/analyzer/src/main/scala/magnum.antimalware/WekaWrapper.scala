package magnum.antimalware

import scala.collection.mutable.ListBuffer
import scala.util.Random

import scala.sys.process._
import scala.language.postfixOps

import scala.io.Source

import java.io.{File,FileWriter}

import weka.core.Instances
import weka.core.converters.ArffLoader
import weka.classifiers.{Classifier,Evaluation}

import weka.classifiers.bayes.{BayesNet,NaiveBayes}
import weka.classifiers.functions.{Logistic,MultilayerPerceptron}
import weka.classifiers.trees.{J48,RandomForest}

class WekaWrapper(fvTag: String, resultsTag: String,
    percentBenignTrain: Int, percentMaliciousTrain: Int, randomSeed: Long) {
  def run() {
    Random.setSeed(randomSeed)

    // Wipe any previous results.
    val resultsDirStr = s"../results/$resultsTag"
    val resultsDir = new File(resultsDirStr)
    org.apache.commons.io.FileUtils.deleteDirectory(resultsDir) 
    resultsDir.mkdir;

    // Get lists of benign and malicious feature vectors.
    val (benignTrainingArffs, benignTestingArffs) =
      getArffs("benign", percentBenignTrain)
    val (maliciousTrainingArffs, maliciousTestingArffs) =
      getArffs("malicious", percentMaliciousTrain)
    val trainingArffs = benignTrainingArffs ++ maliciousTrainingArffs
    val testingArffs = benignTestingArffs ++ maliciousTestingArffs
    val trainingCombinedArff = combineAllArffs(trainingArffs, "training")
    val testingCombinedArff = combineAllArffs(testingArffs, "testing")

    val analysisFile = new FileWriter(s"$resultsDirStr/arff-analysis.txt")
    analysisFile.write("ARFF analysis.\n\n")
    analysisFile.write(s"fvTag: $fvTag\n")
    analysisFile.write(s"resultsTag: $resultsTag\n")
    analysisFile.write(s"percentBenignTrain: $percentBenignTrain\n")
    analysisFile.write(s"percentMaliciousTrain: $percentMaliciousTrain\n\n")
    dumpInfo(benignTrainingArffs ++ benignTestingArffs, analysisFile, "benign")
    dumpInfo(maliciousTrainingArffs ++ maliciousTestingArffs, analysisFile,
      "malicious")
    dumpInfo(benignTrainingArffs ++ benignTestingArffs ++
      maliciousTrainingArffs ++ maliciousTestingArffs, analysisFile, "all")
    analysisFile.write("\n")
    for(line <- Source.fromFile(s"../fv/$fvTag/info.txt").getLines())
      analysisFile.write(line + "\n")
    analysisFile.close

    // Train the classifiers.
    var arffLoader = new ArffLoader
    arffLoader.setFile(trainingCombinedArff)
    val train = arffLoader.getDataSet
    train.setClassIndex(train.numAttributes-1)

    val ttStr = s"$resultsDir/traintest"
    val ttDir = new File(ttStr)
    org.apache.commons.io.FileUtils.deleteDirectory(ttDir) 
    ttDir.mkdir;

    arffLoader.reset
    arffLoader.setFile(testingCombinedArff)
    val test = arffLoader.getDataSet
    test.setClassIndex(test.numAttributes-1)
    collectiveTest(new BayesNet, "BayesNet", train, test, ttStr)
    collectiveTest(new NaiveBayes, "NaiveBayes", train, test, ttStr)
    collectiveTest(new Logistic, "Logistic", train, test, ttStr)
    collectiveTest(new MultilayerPerceptron, "MultilayerPerceptron",
                   train, test, ttStr)
    collectiveTest(new J48, "J48", train, test, ttStr)
    collectiveTest(new RandomForest, "RandomForest", train, test, ttStr)

    individualTest(new BayesNet, "BayesNet", train, testingArffs, ttStr)
    individualTest(new NaiveBayes, "NaiveBayes", train, testingArffs, ttStr)
    individualTest(new Logistic, "Logistic", train, testingArffs, ttStr)
    individualTest(new MultilayerPerceptron, "MultilayerPerceptron",
                   train, testingArffs, ttStr)
    individualTest(new J48, "J48", train, testingArffs, ttStr)
    individualTest(new RandomForest, "RandomForest", train, testingArffs, ttStr)
  }

  // Get the training and testing arffs.
  def getArffs(mode: String, percentTrain: Int):
      (ListBuffer[File], ListBuffer[File]) = {
    val fvDir: File = new File(s"../fv/$fvTag/$mode")
    val unfilteredFvList = fvDir.listFiles

    if (unfilteredFvList == null) {
      print(s"Warning: Fv list for '$mode' is empty.")
      return (ListBuffer[File](), ListBuffer[File]())
    }

    var fvList = new ListBuffer[File]
    var i = 0
    while (i < unfilteredFvList.length) {
      if (unfilteredFvList(i).getName().endsWith(".csv")) {
        fvList += unfilteredFvList(i);
      }
      i += 1
    }

    //val shuffledIndices = util.Random.shuffle(List.range(0, arffList.length))
    var trainingArffs: ListBuffer[File] = new ListBuffer[File]()
    var testingArffs: ListBuffer[File] = new ListBuffer[File]()

    i = 0;
    while (i < (percentTrain/100.0) * fvList.length) {
      //trainingArffs += arffList(shuffledIndices(i))
      trainingArffs += fvList(i)
      i += 1
    }
    while (i < fvList.length) {
      //testingArffs += arffList(shuffledIndices(i))
      testingArffs += fvList(i)
      i += 1
    }

    return (trainingArffs, testingArffs)
  }

  def dumpInfo(files: ListBuffer[File], fw: FileWriter, tag: String) {
    var arffLoader = new ArffLoader

    var fileCount = 0; var vectorCount = 0;
    for (arffFile <- files) {
      if (arffFile.getName().endsWith(".csv")) {
        try {
          arffLoader.setFile(arffFile)
          fileCount += 1
          vectorCount += arffLoader.getDataSet.numInstances
        } catch {
          case e: Exception => {
            println(s"Warning: Exception caught when loading `$arffFile`.")
          }
        } finally {
          arffLoader.reset
        }
      }
    }

    fw.write(s"For $tag apps:\n")
    fw.write(s"  + Number of files: $fileCount\n")
    fw.write(s"  + Number of feature vectors: $vectorCount\n")
    fw.write(s"  + FV per App: ${vectorCount.toDouble/fileCount.toDouble}\n\n")
  }

  def combineAllArffs(files: ListBuffer[File], mode: String): File = {
    var arffLoader = new ArffLoader
    var instances: Instances = null
    var structure: Instances = null

    for (arffFile <- files) {
      try {
        arffLoader.setFile(arffFile)
        if (instances == null) {
          instances = arffLoader.getDataSet
          structure = arffLoader.getStructure
        } else {
          var newInstances = arffLoader.getDataSet
          var i = 0
          while (i < newInstances.numInstances) {
            val instance = newInstances.instance(i)
            instances.add(instance)
            i += 1
          }
        }
      } catch {
        case e: Exception => {
          println(s"Warning: Exception caught when loading `$arffFile`.")
        }
      } finally {
        arffLoader.reset
      }
    }

    val combinedFile = new File(s"../results/$resultsTag/$mode.arff")
    val fw = new FileWriter(combinedFile)
    fw.write(instances.toString)
    fw.close
    return combinedFile
  }

  def collectiveTest(cls: Classifier, name: String,
                     train: Instances, test: Instances, traintest: String) {
    cls.buildClassifier(train)
    var eval = new Evaluation(train)
    eval.evaluateModel(cls, test)
    val title = s"$name\n" +
                 "=========="
    writeEval(title, s"$traintest/${name}Summary.txt", eval)
  }

  def individualTest(cls: Classifier, clsName: String, train: Instances,
      testArffList: ListBuffer[File], ttStr: String) {
    cls.buildClassifier(train)
    var eval = new Evaluation(train)
    var arffLoader = new ArffLoader

    var instances: Instances = null
    var structure: Instances = null

    val testDirStr = s"$ttStr/$clsName"
    val testDir = new File(testDirStr)
    testDir.mkdir;

    for (arffFile <- testArffList) {
      val arffName = arffFile.getName
      arffLoader.setFile(arffFile)
      var instances = arffLoader.getDataSet
      instances.setClassIndex(instances.numAttributes-1)
      eval.evaluateModel(cls, instances)
      val title = s"$clsName - $arffName\n" +
                   "=========="
      writeEval(title, s"$testDirStr/$arffName.txt", eval)
      arffLoader.reset
    }
  }

  private def writeEval(title: String, fileName: String,
                        eval: Evaluation) {
    val file = new File(fileName)
    val fw = new FileWriter(file)
    fw.write(eval.toSummaryString(title, false))
    fw.write("\n")
    fw.write(eval.toClassDetailsString)
    fw.write("\n")
    fw.write(eval.toMatrixString)
    fw.close
  }
}
