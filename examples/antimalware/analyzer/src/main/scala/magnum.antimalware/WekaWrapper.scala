package magnum.antimalware

import scala.collection.mutable.ListBuffer
import scala.util.Random

import scala.sys.process._
import scala.language.postfixOps

import scala.io.Source

import java.io.{File,FileWriter}

import weka.core.Instances
import weka.core.converters.ArffLoader
import weka.classifiers.{Classifier,Evaluation}

import weka.classifiers.bayes.{BayesNet,NaiveBayes}
import weka.classifiers.functions.{Logistic,MultilayerPerceptron}
import weka.classifiers.trees.{J48,RandomForest}

class WekaWrapper(arffTag: String, resultsTag: String,
    percentBenignTrain: Int, percentMaliciousTrain: Int, randomSeed: Long) {
  def run() {
    Random.setSeed(randomSeed)

    // Wipe any previous results.
    val resultsDirStr = s"../results/$resultsTag"
    val resultsDir = new File(resultsDirStr)
    org.apache.commons.io.FileUtils.deleteDirectory(resultsDir) 
    resultsDir.mkdir;

    // Get lists of benign and malicious arffs.
    val (benignTrainingArffs, benignTestingArffs) =
      getArffs("benign", percentBenignTrain)
    val (maliciousTrainingArffs, maliciousTestingArffs) =
      getArffs("malicious", percentMaliciousTrain)
    val trainingArffs = benignTrainingArffs ++ maliciousTrainingArffs
    val testingArffs = benignTestingArffs ++ maliciousTestingArffs
    val trainingCombinedArff = combineAllArffs(trainingArffs, "training")
    val testingCombinedArff = combineAllArffs(testingArffs, "testing")

    // Write a short analysis of the arff files.
    val analysisFile = new FileWriter(s"$resultsDirStr/arff-analysis.txt")
    analysisFile.write("ARFF analysis.\n\n")
    analysisFile.write(s"arffTag: $arffTag\n")
    analysisFile.write(s"resultsTag: $resultsTag\n")
    analysisFile.write(s"percentBenignTrain: $percentBenignTrain\n")
    analysisFile.write(s"percentMaliciousTrain: $percentMaliciousTrain\n\n")
    dumpInfo(benignTrainingArffs ++ benignTestingArffs, analysisFile, "benign")
    dumpInfo(maliciousTrainingArffs ++ maliciousTestingArffs, analysisFile,
      "malicious")
    dumpInfo(benignTrainingArffs ++ benignTestingArffs ++
      maliciousTrainingArffs ++ maliciousTestingArffs, analysisFile, "all")
    analysisFile.write("\n")
    for(line <- Source.fromFile(s"../arff/$arffTag/info.txt").getLines())
      analysisFile.write(line + "\n")
    analysisFile.close

    // Train the classifiers.
    var arffLoader = new ArffLoader
    arffLoader.setFile(trainingCombinedArff)
    val train = arffLoader.getDataSet
    train.setClassIndex(train.numAttributes-1)

    /*
    // Let's remove training completely.
    // This pollutes the training set with testing feature vectors.
    // TODO: Hamilton, confirm we don't need this.
    if (mode == "Training") {
      val cvDir = new File("Results/CV")
      org.apache.commons.io.FileUtils.deleteDirectory(cvDir) 
      cvDir.mkdir

      // TODO Do these need any options?
      crossValidate(new BayesNet, "BayesNet", train)
      crossValidate(new NaiveBayes, "NaiveBayes", train)
      crossValidate(new Logistic, "Logistic", train)
      crossValidate(new MultilayerPerceptron, "MultilayerPerceptron",
                    train)
      crossValidate(new J48, "J48", train)
      crossValidate(new RandomForest, "RandomForest", train)
    } else {
      */
    val ttStr = s"$resultsDir/traintest"
    val ttDir = new File(ttStr)
    org.apache.commons.io.FileUtils.deleteDirectory(ttDir) 
    ttDir.mkdir;

    arffLoader.reset
    arffLoader.setFile(testingCombinedArff)
    val test = arffLoader.getDataSet
    test.setClassIndex(test.numAttributes-1)
    collectiveTest(new BayesNet, "BayesNet", train, test, ttStr)
    collectiveTest(new NaiveBayes, "NaiveBayes", train, test, ttStr)
    collectiveTest(new Logistic, "Logistic", train, test, ttStr)
    collectiveTest(new MultilayerPerceptron, "MultilayerPerceptron",
                   train, test, ttStr)
    collectiveTest(new J48, "J48", train, test, ttStr)
    collectiveTest(new RandomForest, "RandomForest", train, test, ttStr)

    individualTest(new BayesNet, "BayesNet", train, testingArffs, ttStr)
    individualTest(new NaiveBayes, "NaiveBayes", train, testingArffs, ttStr)
    individualTest(new Logistic, "Logistic", train, testingArffs, ttStr)
    individualTest(new MultilayerPerceptron, "MultilayerPerceptron",
                   train, testingArffs, ttStr)
    individualTest(new J48, "J48", train, testingArffs, ttStr)
    individualTest(new RandomForest, "RandomForest", train, testingArffs, ttStr)
  }

  // Get the training and testing arffs.
  def getArffs(mode: String, percentTrain: Int):
      (ListBuffer[File], ListBuffer[File]) = {
    val arffDir: File = new File(s"../arff/$arffTag/$mode")
    val unfilteredArffList = arffDir.listFiles

    if (unfilteredArffList == null) {
      print(s"Warning: Arff list for '$mode' is empty.")
      return (ListBuffer[File](), ListBuffer[File]())
    }

    var arffList = new ListBuffer[File]
    var i = 0
    while (i < unfilteredArffList.length) {
      if (unfilteredArffList(i).getName().endsWith(".arff")) {
        arffList += unfilteredArffList(i);
      }
      i += 1
    }

    val shuffledIndices = util.Random.shuffle(List.range(0, arffList.length))
    var trainingArffs: ListBuffer[File] = new ListBuffer[File]()
    var testingArffs: ListBuffer[File] = new ListBuffer[File]()

    i = 0;
    while (i < (percentTrain/100.0) * arffList.length) {
      trainingArffs += arffList(shuffledIndices(i))
      i += 1
    }
    while (i < arffList.length) {
      testingArffs += arffList(shuffledIndices(i))
      i += 1
    }

    return (trainingArffs, testingArffs)
  }

  def dumpInfo(files: ListBuffer[File], fw: FileWriter, tag: String) {
    var arffLoader = new ArffLoader

    var fileCount = 0; var vectorCount = 0;
    for (arffFile <- files) {
      if (arffFile.getName().endsWith(".arff")) {
        arffLoader.setFile(arffFile)
        fileCount += 1
        vectorCount += arffLoader.getDataSet.numInstances
        arffLoader.reset
      }
    }

    fw.write(s"For $tag apps:\n")
    fw.write(s"  + Number of files: $fileCount\n")
    fw.write(s"  + Number of feature vectors: $vectorCount\n")
    fw.write(s"  + FV per App: ${vectorCount.toDouble/fileCount.toDouble}\n\n")
  }

  def combineAllArffs(files: ListBuffer[File], mode: String): File = {
    var arffLoader = new ArffLoader
    var instances: Instances = null
    var structure: Instances = null

    for (arffFile <- files) {
      arffLoader.setFile(arffFile)
      if (instances == null) {
        instances = arffLoader.getDataSet
        structure = arffLoader.getStructure
      } else {
        var newInstances = arffLoader.getDataSet
        var i = 0
        while (i < newInstances.numInstances) {
          val instance = newInstances.instance(i)
          instances.add(instance)
          i += 1
        }
      }
      arffLoader.reset
    }

    val combinedFile = new File(s"../results/$resultsTag/$mode.arff")
    val fw = new FileWriter(combinedFile)
    fw.write(instances.toString)
    fw.close
    return combinedFile
  }

  /*
  def crossValidate(cls: Classifier, clsName: String, train: Instances) {
    // TODO: Suppress output from this?
    val eval = new Evaluation(train)
    eval.crossValidateModel(cls, train, 10, new Random(1)) //TODO: Seed?
    val title = "10-fold Cross Validation\n" +
                "========================"
    writeEval(title, s"Results/CV/$clsName.txt", eval)
  }
  */

  def collectiveTest(cls: Classifier, name: String,
                     train: Instances, test: Instances, traintest: String) {
    cls.buildClassifier(train)
    var eval = new Evaluation(train)
    eval.evaluateModel(cls, test)
    val title = s"$name\n" +
                 "=========="
    writeEval(title, s"$traintest/${name}Summary.txt", eval)
  }

  def individualTest(cls: Classifier, clsName: String, train: Instances,
      testArffList: ListBuffer[File], ttStr: String) {
    cls.buildClassifier(train)
    var eval = new Evaluation(train)
    var arffLoader = new ArffLoader

    var instances: Instances = null
    var structure: Instances = null

    val testDirStr = s"$ttStr/$clsName"
    val testDir = new File(testDirStr)
    testDir.mkdir;

    for (arffFile <- testArffList) {
      val arffName = arffFile.getName
      arffLoader.setFile(arffFile)
      var instances = arffLoader.getDataSet
      instances.setClassIndex(instances.numAttributes-1)
      eval.evaluateModel(cls, instances)
      val title = s"$clsName - $arffName\n" +
                   "=========="
      writeEval(title, s"$testDirStr/$arffName.txt", eval)
      arffLoader.reset
    }
  }

  private def writeEval(title: String, fileName: String,
                        eval: Evaluation) {
    val file = new File(fileName)
    val fw = new FileWriter(file)
    fw.write(eval.toSummaryString(title, false))
    fw.write("\n")
    fw.write(eval.toClassDetailsString)
    fw.write("\n")
    fw.write(eval.toMatrixString)
    fw.close
  }
}
